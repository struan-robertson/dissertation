#+title: Terrain Model Processing with Machine Learning
#+AUTHOR: Struan Robertson @@latex:\\@@ BSc (Hons) Applied Computing
#+DATE: May 2023
#+property: header-args :session paper :exports results :eval never-export
#+BIBLIOGRAPHY: library.bib
#+OPTIONS: toc:nil
#+LANGUAGE: en-gb
#+cite_export: biblatex

#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [twocolumn]
# Styles

# Basic Packages
#+LaTeX_HEADER: \usepackage{balance}
#+LaTeX_HEADER: \usepackage{graphics}
#+LaTeX_HEADER: \usepackage{txfonts}
#+LaTeX_HEADER: \usepackage{times}
#+LaTeX_HEADER: \usepackage{color}
#+LaTeX_HEADER: \usepackage{textcomp}
#+LaTeX_HEADER: \usepackage{booktabs}
#+LaTeX_HEADER: \usepackage{todonotes}
#+LaTeX_HEADER: \usepackage{float}
#+LaTeX_HEADER: \usepackage{url}
#+LaTeX_HEADER: \usepackage{titling}
#+LaTeX_HEADER: \usepackage[left=3cm,right=2cm,top=2.5cm,bottom=2cm]{geometry}
#+LaTeX_HEADER: \usepackage[british]{babel}

# Font sizes
#+LaTeX_HEADER: \usepackage{sectsty}
#+LaTeX_HEADER: \sectionfont{\Large}
#+LaTeX_HEADER: \subsectionfont{\large}
#+LaTeX_HEADER: \subsubsectionfont{\large}
#+LaTeX_HEADER: \paragraphfont{\normalsize}

# Positioning
#+LaTeX_HEADER: \setlength{\parindent}{0em}
#+LaTeX_HEADER: \setlength{\parskip}{1em}
#+LaTeX_HEADER: \setlength{\columnsep}{2em}
#+LaTeX_HEADER: \setlength{\droptitle}{-5em}

# Define global style for URLs
#+LaTeX_HEADER: \makeatletter
#+LaTeX_HEADER: \def\url@leostyle{%
#+LaTeX_HEADER:     \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
#+LaTeX_HEADER: \makeatother
#+LaTeX_HEADER: \urlstyle{leo}

#+LaTeX_HEADER: \usepackage[
#+LaTeX_HEADER:    %backend=biber,
#+LaTeX_HEADER:    natbib=true,
#+LaTeX_HEADER:    style=numeric,
#+LaTeX_HEADER:    sorting=none
#+LaTeX_HEADER: ]{biblatex}

#+LATEX: \begin{abstract}

With the launch of the lunar orbiter laser altimeter (LOLA) on NASA's lunar reconnaissance orbiter (LRO), a large amount of high-resolution digital elevation maps (DEMs) have been constructed, providing a precise topographical model of the moons surface.
These DEMs are prone to voids containing no data, making the map less reliable for scientific purposes and future moon missions.
This paper uses a machine learning model to allow for the technique of image inpainting to be used with lunar DEMs.
Image inpainting uses pixel data from an image to generate missing data to fill a void.
DEMs can be thought of mathematically as identical to a single channel (greyscale) image, a two-dimensional array with "pixel" values corresponding to height, and so the technique of image inpainting can be easily applied to DEMs.
A Generative Adversarial Network (GAN) based on a fully convolutional architecture was used for the inpainting.


#+LATEX: \end{abstract}

* Introduction

Sensory data from the Lunar Orbiter Laser Altimeter (LOLA) and Lunar Reconnaissance Orbiter Camera (LROC) has been used for the construction of Lunar digital elevation models (DEMs) since the launch of the Lunar Reconnaissance Orbiter (LRO) in 2009.
LRO has several primary objectives as part of a series of robotic missions that aim to pave the way for a permanent human presence on the Moon, including determining the global topography of the lunar surface at meter-scale resolution.
Topographical data from LOLA and LROC will be used to facilitate the selection of future landing sites, so accuracy and completeness are essential.
[cite:@chinLunarReconnaissanceOrbiter2007]

The LRO is in a polar orbit around the moon, scanning the surface in swathes 50 to 60m wide, with an average separation ranging between 1.2km and 200m depending on the position of LRO in orbit.
LOLA uses a laser altimeter to measure the distance from LRO to the lunar surface at 5 different spots simultaneously, providing DEMs ranging from ~30m resolution at the equator to ~5m resolution at the poles. [cite:@smithLunarOrbiterLaser2010].
LROC uses two narrow-angle cameras (NACs) to collect stereo observations at a resolution of 0.5 to 1.5 m per pixel.
These high resolution images can be used to generate high resolution (~5m at the equator) DEMS, referred to as NAC DEMs [cite:@tranGeneratingDigitalTerrain2010].

Both LOLA and NAC dems are prone to no-data voids resulting from shadowed regions (NAC) or terrain features blocking the return of the laser altimeter (LOLA).
As the LRO has a polar orbit, data is recorded in strips, which must be joined together to create larger DEMs. These strips are not immediately adjacent to each other, resulting in a no-data void in-between.
Reconstructing these no-data voids is non-trivial, with Park and Choi [cite:@parkNeuralProcessApproach2021]  noting the following challenges:
 - NAC DEMs require high-resolution reconstruction methodology
 - NAC DEMs are large and high-resolution area maps, thus a scalable approach should be applied
 - The reconstruction algorithm must be reliable since it can affect related lunar studies or exploration missions

Traditional algorithmic methods for correcting no-data voids within DEMs include inverse distance weight method (IDW), local polynomial interpolation method (LPI), spline with tension method (ST) and other algorithms to interpolate the elevation sampling points. Interpolation methods use the neighboring elevation values for void infilling, thus performance is directly proportional to the size of the void.
Voids of any significant size result in interpolation methods returning inaccurate reconstruction results.  [cite:@reuterEvaluationVoidFilling2007]

Within the field of computer vision, the problem of image inpainting fundamentally seeks to solve the same issue as DEM void infilling; a 2-dimensional grid of points with an area of no data which must be inferred from surrounding points.
RGB images contain three channels whereas a DEM contains only one, however inpainting techniques process each channel independently and then combine the results, so inpainting a DEM is functionally identical to inpainting a greyscale image.
Generative adversarial networks (GANs)[cite:@goodfellowGenerativeAdversarialNetworks2020] are a deep learning generative model that when constructed with deep convolutional neural networks[cite:@krizhevskyImageNetClassificationDeep2017] have been shown to have excellent performance in image inpainting[cite:@pathakContextEncodersFeature2016;@yuGenerativeImageInpainting2018] and have been successfully applied to the task of DEM void infilling[cite:@gavriilVoidFillingDigital2019;@zhangVoidFillingBased2020;@qiuVoidFillingDigital2019].

This papers implements a GAN inpainting network based on the structure described by Yu /et al./[cite:@yuGenerativeImageInpainting2018] and shows it to effectively pass the first two challenges set forward by Park and Choi[cite:@parkNeuralProcessApproach2021].
The final challenge of DEM reliability may be passed by this network, however this is difficult to test as GANs excel at infilling /plausible/ data, yet the /accuracy/ of this data may not be necessarily sufficient.


* Background
** Digital Elevation Maps

Digital elevation maps (DEMs) are digital representations of terrain elevation (Figure [[fig:apollo_17]]).
A DEM consists of a grid of cells, where each cell represents a specific area of terrain and contains the elevation value of the ground at this location.
The cells of a DEM can also be described as pixels, as they represent a discrete location in a raster grid.
DEM gridsize refers to the size of cells and can be thought of as the resolution of the DEM.
Gridsize can vary depending on the source of data and the intended application, with common grid sizes ranging from a few meters to several kilometers.

#+NAME: apollo_17
#+begin_src jupyter-python :file images/apollo_17.png :eval never-export
import numpy as np
from osgeo import gdal

dem = '/home/struan/Development/Inpaint/Final Network/data/NAC_DTM_APOLLO17.TIF'
dem = gdal.Open(dem)

dem_arr = dem.ReadAsArray()
threshold = -10000
dem_arr[dem_arr < threshold] = np.nan

plt.imshow(dem_arr, cmap='terrain')
plt.axis('off')
plt.colorbar()
plt.show()
#+end_src

#+CAPTION: Shaded DEM of Apollo 17 landing site in Taurus-Littrow Valley
#+NAME: fig:apollo_17
#+RESULTS: apollo_17
[[file:images/apollo_17.png]]

#+NAME: dem_and_slope
#+begin_src jupyter-python :file images/dem_and_slope.png
import external
import numpy as np
import matplotlib.pyplot as plt

dem = '/home/struan/Development/Inpaint/Final Network/datac/NAC_DTM_APOLLO17.TIF'
tiled = external.tile(dem, (256,256))

tile_n = 7
t = tiled[0][tile_n]
t = t.cpu().detach().numpy()
t = np.transpose(t, (1, 2, 0))

plt.figure(figsize=(8,10))
plt.subplot(1,2,1)
plt.imshow(t[:,:,0], cmap='terrain')
plt.title("DEM")
plt.axis('off')
plt.subplot(1,2,2)
plt.imshow(t[:,:,1], cmap='viridis')
plt.title("Slope")
plt.axis('off')
plt.show()
#+end_src

#+CAPTION: Section of DEM with computed slope
#+NAME: fig:dem_and_slope
#+RESULTS: dem_and_slope
[[file:images/dem_and_slope.png]]

The slope of a DEM refers to the steepness of terrain at each location in the map (Figure [[fig:dem_and_slope]])
Slope is calculated by traversing a 3 x 3 window (Figure [[fig:window]]) over the DEM[cite:@qiuVoidFillingDigital2019].
The slope value at the central pixel /e/ can be calculated by using the algorithm proposed by Horn /et al./[cite:@hornHillShadingReflectance1981] :
\begin{align}
Slope &= arctan\sqrt{Slope^2_{we} + Slope^2_{sn}}, \\
Slope_{we} &= \frac{(e_8 + 2e_1 + e_5) - (e_7 + 2e_3 + e_6)}{8 \times Gridsize}, \\
Slope_{sn} &= \frac{(e_7 + 2e_4 + e_8) - (e_6 + 2e_2 + e_5)}{8 \times Gridsize},
\end{align}

#+CAPTION: The 3x3 moving window[cite:@qiuVoidFillingDigital2019]
#+NAME: fig:window
#+ATTR_LATEX: :width 4cm
[[file:images/window.png]]

The most common data format for the storage of DEMs is GeoTiff.
A GeoTiff is a type of TIFF (Tagged Image File Format) that with the raw DEM raster data also stores spatial metadata such as pixel resolution (gridsize).
Lunar DEMs are also commonly stored in NASA's PDS (Planetary Data System) archival formats PDS3 and PDS4.
PDS is used to archive multiple kinds of data from planetary science missions, not just DEMs.
Older missions (pre 2011) are typically archived in PDS3, with post 2011 missions using PDS4.
Although stored differently, the raster data in PDS files and GeoTiffs is identical.

The issue of no-data voids is not limited to lunar DEMs, as DEMs are typically constructed using remote sensing technology which is prone to the same errors.

** Deep Neural Networks

Neural networks are a type of machine learning algorithm that is loosely modeled after the structure and function of biological brains, consisting of multiple artificial neurons[cite:@grossiIntroductionArtificialNeural2008].
A neuron can hold any value, however in most neural networks this value is restricted between 0 and 1 or -1 and 1.
The value a neuron holds is referred to as its activation.
When data is passing forwards through a network, each neuron has an activation determined by the input data.
In a fully connected network, these neurons are arranged into layers, with every neuron in a layer connected to every neuron in the previous layer (Figure [[fig:neural_network]]) .
The first layer is the input layer, the last the output layer and the layers in-between are hidden layers.
In image processing tasks, such as image inpainting, the neurons in the input layer correspond to the pixels of the input image.
The layered structure of the neural network is highly efficient as it allows the network to break down complex problems into smaller steps.

#+CAPTION: Simple feedforward artifical neural network[cite:@ArtificialNeuralNetwork2023]
#+NAME: fig:neural_network
[[file:images/neural_network.png]]

Each connection between neurons in different layers has an associated weight.
This weight is an indication of how the neuron in the second layer is correlated to the neuron in the first.
A positive weight indicates that when the first neuron has a high activation so should the second, and a negative weight the inverse.
Each neuron also holds a value called a bias, which can be thought of as the minimum weighted sum for the neuron to activate.
To compute the activation of a second layer neuron, take the sum of the activations of the first layer neurons multiplied with their weights and add the bias (Equation [[eqn:activation]]).
The activation can be any number, however to normalise the signal between a range and add non-linearity to the network the activation is passed through an activation function.

ReLU (Rectified Linear Unit) is an activation function which can introduce sparsity into the network, meaning only a subset of neurons will be activated for any given input.
The constant gradient of ReLU when the gradient is positive improves the stability of gradients in the network, making vanishing gradients less likely than other activation functions, such as sigmoid.
ReLU also introduces sparsity in the activations since it outputs zero for negative input values.
Whilst this can simplify the network and reduce computational complexity, it can also lead to the "dying ReLU" problem, where some neurons in the network stop contributing to the output due to always receiving negative input values and having an output of zero.
The ELU activation function (Figure [[fig:ELU]]) becomes smooth slowly until its output equals $-\alpha$ for negative inputs, ensuring all neurons in the network can contribute to the output even if their inputs are negative.
This can improve the performance of the network when dealing with noisy or outlier data, which is very common in DEMs.
For this reason ELU is the most common activation function in the network described by this paper.
Other activation functions used are Leaky ReLU and Tanh.
Leaky ReLU also addresses the dying ReLU problem, however instead of a smooth curve to $-\alpha$, negative values have a small constant negative slope equal to $\alpha$ (usually 0.1).
Leaky ReLU is used in the discriminator of this the network described by this paper as it allows the activation to be infinitely small.
In generative networks such as the generator, this can lead to vanishing gradients, however in classifier networks such as the discriminator it is important for it to be able to learn negative associations.
The Tanh activation function compresses all activations to between -1 and 1, and so is used as the final layer in generative networks to produce output data of the same range as the input data.

#+NAME: eqn:activation
\begin{equation}
a^{(1)}_0 = ELU(w_{0,0}a^{(0)}_0 + w_{0,1}a^{(0)}_1 + \cdots + w_{0,n}a^{(0)}_n + b_0)
\end{equation}

#+NAME: ELU
#+begin_src jupyter-python :file images/ELU.png
import matplotlib.pyplot as plt
import numpy as np

def elu(x, alpha=1):
    return x if x >= 0 else alpha * (np.exp(x) - 1)

inputs = np.linspace(-8, 8, 1000)
outputs = [elu(x) for x in inputs]

plt.plot(inputs, outputs)
plt.xlabel('Input')
plt.ylabel('Output')
plt.title('ELU Function (Î±=1)')
plt.axhline(0, color='black', linewidth=.5)
plt.axvline(0, color='black', linewidth=.5)
plt.grid()
plt.show()
#+end_src

#+CAPTION: ELU activation function
#+NAME: fig:ELU
#+RESULTS: ELU
[[file:images/ELU.png]]


As the equations are linear, to efficiently compute the activation of every neuron in a forward layer, the equations can be stacked into matrices[cite:@3Blue1BrownWhatNeural] :
\begin{equation}
\begin{bmatrix} a^{(1)}_0 \\ a^{(1)}_1 \\ \vdots \\ a_n^{(1)} \end{bmatrix} = ELU \left( \begin{bmatrix}w_{0,0} & w_{0,1} & \dots & w_{0,n} \\ w_{1,0} & w_{1,1} & \dots & w_{1,n} \\ \vdots & \vdots & \ddots & \vdots \\ w_{k,0} & w_{k,1} & \dots & w_{k,n} \end{bmatrix} \begin{bmatrix} a_0^{(0)} \\ a_1^{(0)} \\ \vdots \\ a_n^{(0)} \end{bmatrix} + \begin{bmatrix} b_0 \\ b_1 \\ \vdots \\ b_n \end{bmatrix} \right)
\end{equation}

A cost function such as Mean Squared Error (MSE) is used to measure how well the network is performing.
As the network is itself a function, the cost function is a function which takes all the weights and biases of the network as inputs and returns a value describing how well these weights and biases perform.
A neural network is trained with the following steps.
Input data is propagated forwards through the network layer by layer.
The cost function is then evaluated using the predicted output and the actual output, with the error between the two values calculated.
The error is then backpropagated through the network, layer by layer, starting from the output layer.
The desired output of the output layer is known, so by working backwards layer by layer the activations of each neuron that would have resulted in the desired output can be calculated.
The error at each layer is used to calculate the gradient of the cost function with respect to the weights of that layer[cite:@leTutorialDeepLearning2015].
The weights and biases of the network are updated using the gradients calculated during backpropagation by using an optimisation algorithm such as stochastic gradient descent (SGD)[cite:@ruderOverviewGradientDescent2016], which adjusts the weights and biases in a way that takes a step down the gradient towards a local minimum of the cost function; with the steeper the gradient the greater the step taken.
The network can become stuck in a local minimum, as it is impossible to know what the true minimum is, only the downwards direction is known.
An analogy for this would be rolling a ball down a hill.

As calculating the gradient for the entire dataset is very computationally difficult, the data is batched, with the cost function calculated for each example in a batch and then averaged to get a single cost value for the batch - which is then backpropagated.
This average is important, as the ideal adjustment to weights and biases will be different for each piece of input data, so by averaging the cost function of each a generalised value is reached.
An epoch is the entire set of training data. It normally takes multiple epochs of training data for the network to converge at a set of weights that minimise the cost function.

A deep neural network is functionally the same, however it involves more hidden layers than the classical network described above.

*** Convolutional Neural Networks

#+CAPTION: Convolution step[cite:@ConvolutionalNeuralNetworks]
#+NAME: fig:convolution
[[file:images/convolution.png]]

A convolutional neural network (CNN) is comprised of layers of 2D convolutions.
These layers consist of filters which themselves are comprised of kernels, small matrices with learned weights as values[cite:@osheaIntroductionConvolutionalNeural2015].
Filters have a kernel for each input channel to the layer, with each kernel moving accross the channel and performing an elementwise multiplication with the part of the input it is currently on (Figure [[fig:convolution]]).
The results of all kernels in a filter are summed into a single output pixel, meaning that each filter produces one output channel.
The stride of the layer determines how far the filter moves over the data every convolution, therefore a stride greater than one reduces the spatial dimensions by a factor of the stride size[cite:@dumoulinGuideConvolutionArithmetic2018].
The inverse is also true, a sub-pixel stride of less than one increases the spatial dimensions, however this can lead to checkerboard artifacts where kernels overlap so a more appropriate technique is to interpolate the image into a larger size and then convolve over it, referred to as a resize-convolution[cite:@odenaDeconvolutionCheckerboardArtifacts2016][cite:@aitkenCheckerboardArtifactFree2017]

#+CAPTION: Visualised convolution filters, with increasing complexity of features extracted[cite:@graetzHowVisualizeConvolutional2019]
#+NAME: fig:convolution_filter
[[file:images/convolution_filter.png]]

Each kernel is unique, with the values of the matrix being the weights learned by through training.
Filters have a bias, which gets added to all values in the output data.
By using multiple filters with a stride greater than 1, the number of output channels can be increased whilst the spatial dimensions of the data is decreased.
This is a fundamental pattern in a CNN, as it allows for kernels to learn to extract features.
By reducing the spatial dimensions of the image, earlier layers extract low level features which get combined by following layers (Figure [[fig:convolution_filter]]).
The compression of the input data also allows for later kernels to extract patterns from an area much larger than their kernel size .
Dilated convolutions expand the kernel by inserting holes between its elements, allowing the kernel to cover a larger area than its size[cite:@dumoulinGuideConvolutionArithmetic2018].


#+CAPTION: Simplified diagram of an autoencoder[cite:@birlaAutoencoders2019]
#+NAME: fig:autoencoder
[[file:images/autoencoder.png]]

*Autoencoders*

Autoencoders construct an encoder and decoder out of convolutional layers, using the change in channel number and spatial dimensions to learn to deconstruct then reconstruction data (Figure [[fig:autoencoder]]).
The encoder is trained to reduce the spatial dimensions of the input data whilst increasing the number of channels.
The latent space is the result of this encoding, a lower-dimensional compressed representation of the data.
In a trained network, the latent space captures the most important features and patterns of the input data in a compact and efficient way[cite:@michelucciIntroductionAutoencoders2022].
This representation is generalised, two different craters would be represented as craters, even if they had visual differences.
Autoencoders are useful for image inpainting as the latent space more clearly demonstrates the missing parts of features.
Dilated convolutions are especially effective in latent space, as the kernel acts over a larger area of the already compressed data for little computational cost, allowing it to learn complex and large features.
The decoder is trained to translate the latent space back into the inpainted image.
A famous use of autoencoders is in early "deepfake" networks, which are designed to swap faces in images.
In a deepfake network, separate autoencoders are trained for the two faces.
By swapping the decoder from one autoencoder into another, the autoencoder encodes properties (such face angle) for one face, however decodes the image with the other face.


*** Generative Adversarial Networks
Generative adversarial networks (GANs)[cite:@goodfellowGenerativeAdversarialNetworks2020] are a machine learning framework based on game theory.
They are constructed from two opposing networks, a generator and a discriminator.
The generator learns to generate fake data and attempts to trick the discriminator, which learns to distinguish between real and fake samples.
The adversarial loss between the competing networks is able to catch errors which would be overlooked by other loss functions, such as mean squared error[cite:@lotterUnsupervisedLearningVisual2016].

A difficult challenge of training GANs is keeping the training of both generator and discriminator balanced.
If one network becomes substantially better than the other, the gradient for the better network will explode and the other networks gradient will vanish.
Wasserstein GANs[cite:@arjovskyWassersteinGenerativeAdversarial2017] improve this situation by using the Wasserstein-1 distance to measure discrepancy between real and generated data distributions.
The Wasserstein-1 distance is a measure of how much effort it would take to move one probability density into another, which is continuous in nature, leading to more stable gradients.
To further improve the stability of the network, a gradient penalty can be applied to the network to ensure that both generator and discriminator train at similar rates.
If one trains faster than the other, the gradient descent is penalised to allow the other to close the gap.

A GAN containing only convolutional layers is referred to as a deep convolutional GAN[cite:@radfordUnsupervisedRepresentationLearning2016].
This simplification of the network is more computationally efficient than the GAN first proposed by Goodfellow et al.[cite:@goodfellowGenerativeAdversarialNetworks2020], whilst allowing for deeper models and increased image resolution.
GANs typically use encoders and decoders, as novel data can be hallucinated in the latent space and then decoded to the output.
The addition of dilated convolutions[cite:@yuMultiScaleContextAggregation2016] as four layers in the latent space adds an enhanced receptive field, allowing for improved feature learning.

GAN training is unsupervised, meaning that it the training data set does not need to be labeled.
This is because data is known to be real or generated, and so the loss function can operate independently.
As training neural networks requires large amounts of data, unsupervised training requires far less human effort to achieve.
GANs are well suited for the task of void infilling, as the adversarial loss results in the generator being trained to generate the accurate DEMs, without the need of manually labeling voids locations.

#+CAPTION:Contextual attention focusing network on cat [cite:@zhangAgileAmuletRealTime2018]
#+NAME: fig:contextual_attention
[[file:images/contextual_attention.png]]

*Contextual Attention*

Image features are extracted in convolutional neural networks with local kernels layer by layer.
This locality reduces the kernels effectiveness at borrowing from distant spatial locations; there may be many layers of convolutions reducing image spatial dimensions before data from other parts of the image become relevant to a kernel.
Yu et al.[cite:@yuGenerativeImageInpainting2018] proposed a novel contextual attention layer in the deep generative network to remedy this issue.
This layer learns where to copy feature information from in the known background (non-masked part of the image) to generate missing patches (Figure [[fig:contextual_attention]]).
This concept is inspired by human attention, people selectively attend to specific aspects of the the environment based on relevance or saliency.
As the layer is differentiable it can be trained, improving the efficiency of the network as a whole.
The fully-convolutional nature of the contextual attention layer results in it being highly effective in image inpainting GANs.

** Related Work

/A Neural Process Approach for Probabilistic Reconstruction of No-Data Gaps in Lunar Digital Elevation Maps/ by Park and Choi[cite:@parkNeuralProcessApproach2021] is the most relevant work to this paper, as it also attempts to solve the issue of lunar DEM void reconstruction.
They use a sparse attentive neural processes (SANPs) (a novel implementation of attentive neural processes[cite:@kimAttentiveNeuralProcesses2019]) to reduce complexity and prevent over-fitting.
This works on a similar concept to the contextual attention layer proposed by Yu /et al./[cite:@yuGenerativeImageInpainting2018] and used in the current paper; training the network to identify regions in the input image that are more or less important for the infilling task.
Due to being fully convolutional in nature, contextual attention layers are likely more effective at improving void infilling GANs, with strong inpainting results described by Yu /et al./[cite:@yuGenerativeImageInpainting2018].

A problem of void infilling GANs is that whilst a trained GAN produces plausible output data, it is impossible to asses the data accuracy, severely hampering the ability to use this data in future scientific missions.
To overcome this issue, Park and Choi[cite:@parkNeuralProcessApproach2021] implement uncertainty analysis in their void filling network.
This produces uncertainty maps of infilled regions, which indicate how confident the network is of each pixel.
Whilst the network proposed in the current paper likely produces more accurate void infilling results than Park and Choi[cite:@parkNeuralProcessApproach2021] - due to the use of contextual attention and slope data - uncertainty maps make infilled data more useful in real life applications.
In future work, uncertainty analysis could be implemented in the network described in this paper.

[cite:@zhangVoidFillingBased2020]

[cite:@dongFillingVoidsElevation2020]

[cite:@stolzleReconstructingOccludedElevation2022]

[cite:@qiuVoidFillingDigital2019]


* Methodology
**  Problem Formation and Notation
** Deep Generational Model
*** Deep Generational Model Structure
Include Contextual Attention layer

#+ATTR_LATEX: :float multicolumn
[[file:images/gan_architecture.png]]

#+ATTR_LATEX: :float multicolumn
[[file:images/critic_architecture.png]]

*** Deep Generational Model Loss Functions
Maybe swap ReLU here
** Unified Inpainting Network
** Data Post-processing
*** Poisson Blending
Due to the similarity with images, image processing techniques can be applied to the DEMs.
In this paper, the technique of poisson seamless cloning[cite:@perezPoissonImageEditing2003] was used as a post processing step to remove any boundary between the infilled area and original DEM.

*** Gaussian Blur


* Experiments
** Model Training
Talk about all the changes made to the network
** Model Testing Methodology


* Results

#+begin_src jupyter-python :file images/l1_loss.png
import matplotlib.pyplot as plt
import numpy as np
from external import load_losses

losses = load_losses()

epoch = losses[0]
l1 = losses[1]

plt.scatter(epoch, l1, s=2.5)
a, b = np.polyfit(epoch, l1, 1)
plt.plot(epoch, a * np.array(epoch) + b, color='black')

plt.xlabel('Epoch')
plt.ylabel('L1 Loss')
plt.show()
#+end_src

#+RESULTS:
[[file:images/l1_loss.png]]

#+begin_src jupyter-python :file images/ae_loss.png
import matplotlib.pyplot as plt
import numpy as np
from external import load_losses

losses = load_losses()

epoch = losses[0]
ae = losses[2]

plt.scatter(epoch, ae, s=2.5)
a, b = np.polyfit(epoch, ae, 1)
plt.plot(epoch, a * np.array(epoch) + b, color='black')

plt.xlabel('Epoch')
plt.ylabel('Autoencoder Loss')
plt.show()
#+end_src

#+RESULTS:
[[file:images/ae_loss.png]]

#+begin_src jupyter-python :file images/wasserstein_loss.png
import matplotlib.pyplot as plt
import numpy as np
from external import load_losses

losses = load_losses()

epoch = losses[0]
wgan_g = losses[3]
wgan_d = losses[4]

plt.scatter(epoch, wgan_g, label='Wasserstein Generator Loss', s=2.5)
plt.scatter(epoch, wgan_d, label='Wasserstein Discriminator Loss', s=2.5)

a, b = np.polyfit(epoch, wgan_g, 1)
plt.plot(epoch, a * np.array(epoch) + b, color='black')

a, b = np.polyfit(epoch, wgan_d, 1)
plt.plot(epoch, a * np.array(epoch) + b, color='black')

plt.xlabel('Epoch')
plt.ylabel('Wasserstein Distance')

plt.legend()
plt.show()
#+end_src

#+RESULTS:
[[file:images/wasserstein_loss.png]]


* Discussion

* Conclusion

* Future Work

#+LATEX: \section*{Acknowledgements}

#+PRINT_BIBLIOGRAPHY:

#+LATEX: \section*{Appendices}
