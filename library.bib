@misc{aitkenCheckerboardArtifactFree2017,
  title = {Checkerboard Artifact Free Sub-Pixel Convolution: {{A}} Note on Sub-Pixel Convolution, Resize Convolution and Convolution Resize},
  shorttitle = {Checkerboard Artifact Free Sub-Pixel Convolution},
  author = {Aitken, Andrew and Ledig, Christian and Theis, Lucas and Caballero, Jose and Wang, Zehan and Shi, Wenzhe},
  year = {2017},
  month = jul,
  number = {arXiv:1707.02937},
  eprint = {arXiv:1707.02937},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {The most prominent problem associated with the deconvolution layer is the presence of checkerboard artifacts in output images and dense labels. To combat this problem, smoothness constraints, post processing and different architecture designs have been proposed. Odena et al. highlight three sources of checkerboard artifacts: deconvolution overlap, random initialization and loss functions. In this note, we proposed an initialization method for sub-pixel convolution known as convolution NN resize. Compared to sub-pixel convolution initialized with schemes designed for standard convolution kernels, it is free from checkerboard artifacts immediately after initialization. Compared to resize convolution, at the same computational complexity, it has more modelling power and converges to solutions with smaller test errors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/struan/Zotero/storage/DHEP9SAQ/Aitken et al. - 2017 - Checkerboard artifact free sub-pixel convolution .pdf;/home/struan/Zotero/storage/52GJKXLW/1707.html}
}

@article{ArtificialNeuralNetwork2023,
  title = {Artificial Neural Network},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-04-11},
  abstract = {Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets, are computing systems inspired by the biological neural networks that constitute animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1149334703},
  file = {/home/struan/Zotero/storage/BLNDHUMY/Artificial_neural_network.html}
}

@article{chinLunarReconnaissanceOrbiter2007,
  title = {Lunar {{Reconnaissance Orbiter Overview}}: {{The~Instrument Suite}} and {{Mission}}},
  shorttitle = {Lunar {{Reconnaissance Orbiter Overview}}},
  author = {Chin, Gordon and Brylow, Scott and Foote, Marc and Garvin, James and Kasper, Justin and Keller, John and Litvak, Maxim and Mitrofanov, Igor and Paige, David and Raney, Keith and Robinson, Mark and Sanin, Anton and Smith, David and Spence, Harlan and Spudis, Paul and Stern, S. Alan and Zuber, Maria},
  year = {2007},
  month = apr,
  journal = {Space Science Reviews},
  volume = {129},
  number = {4},
  pages = {391--419},
  issn = {1572-9672},
  doi = {10.1007/s11214-007-9153-y},
  urldate = {2023-03-24},
  abstract = {NASA's Lunar Precursor Robotic Program (LPRP), formulated in response to the President's Vision for Space Exploration, will execute a series of robotic missions that will pave the way for eventual permanent human presence on the Moon. The Lunar Reconnaissance Orbiter (LRO) is first in this series of LPRP missions, and plans to launch in October of 2008 for at least one year of operation. LRO will employ six individual instruments to produce accurate maps and high-resolution images of future landing sites, to assess potential lunar resources, and to characterize the radiation environment. LRO will also test the feasibility of one advanced technology demonstration package. The LRO payload includes: Lunar Orbiter Laser Altimeter (LOLA) which will determine the global topography of the lunar surface at high resolution, measure landing site slopes, surface roughness, and search for possible polar surface ice in shadowed regions, Lunar Reconnaissance Orbiter Camera (LROC) which will acquire targeted narrow angle images of the lunar surface capable of resolving meter-scale features to support landing site selection, as well as wide-angle images to characterize polar illumination conditions and to identify potential resources, Lunar Exploration Neutron Detector (LEND) which will map the flux of neutrons from the lunar surface to search for evidence of water ice, and will provide space radiation environment measurements that may be useful for future human exploration, Diviner Lunar Radiometer Experiment (DLRE) which will chart the temperature of the entire lunar surface at approximately 300 meter horizontal resolution to identify cold-traps and potential ice deposits, Lyman-Alpha Mapping Project (LAMP) which will map the entire lunar surface in the far ultraviolet. LAMP will search for surface ice and frost in the polar regions and provide images of permanently shadowed regions illuminated only by starlight. Cosmic Ray Telescope for the Effects of Radiation (CRaTER), which will investigate the effect of galactic cosmic rays on tissue-equivalent plastics as a constraint on models of biological response to background space radiation. The technology demonstration is an advanced radar (mini-RF) that will demonstrate X- and S-band radar imaging and interferometry using light weight synthetic aperture radar. This paper will give an introduction to each of these instruments and an overview of their objectives.},
  langid = {english},
  keywords = {Lunar,Moon,NASA,Remote observation,Space instrumentation,Spacecraft,Vision for Space Exploration},
  file = {/home/struan/Zotero/storage/Z3QU972N/Chin et al. - 2007 - Lunar Reconnaissance Orbiter Overview The Instrum.pdf}
}

@article{demirayDSRGANSuperResolutionGenerative2021,
  title = {D-{{SRGAN}}: {{DEM Super-Resolution}}~with {{Generative Adversarial Networks}}},
  shorttitle = {D-{{SRGAN}}},
  author = {Demiray, Bekir Z. and Sit, Muhammed and Demir, Ibrahim},
  year = {2021},
  month = jan,
  journal = {SN Computer Science},
  volume = {2},
  number = {1},
  pages = {48},
  issn = {2661-8907},
  doi = {10.1007/s42979-020-00442-2},
  urldate = {2023-02-28},
  abstract = {Digital elevation model (DEM) is a critical data source for variety of applications such as road extraction, hydrological modeling, flood mapping, and many geospatial studies. The usage of high-resolution DEMs as inputs in many application areas improves the overall reliability and accuracy of the raw dataset. The goal of this study is to develop a machine learning model that increases the spatial resolution of DEM without additional information. In this paper, a GAN based model (D-SRGAN), inspired by single image super-resolution methods, is developed and evaluated to increase the resolution of DEMs. The experiment results show that D-SRGAN produces promising results while constructing 3~feet high-resolution DEMs from 50~feet low-resolution DEMs. It outperforms common statistical interpolation methods and neural network algorithms.This study shows that it is possible to use the power of artificial neural networks to increase the resolution of the DEMs. The study also demonstrates that approaches from single image super-resolution can be applied for DEM super-resolution.},
  langid = {english},
  file = {/home/struan/Zotero/storage/BFEHDYSB/Demiray et al. - 2021 - D-SRGAN DEM Super-Resolution with Generative Adve.pdf}
}

@misc{devarakondaAdaBatchAdaptiveBatch2018,
  title = {{{AdaBatch}}: {{Adaptive Batch Sizes}} for {{Training Deep Neural Networks}}},
  shorttitle = {{{AdaBatch}}},
  author = {Devarakonda, Aditya and Naumov, Maxim and Garland, Michael},
  year = {2018},
  month = feb,
  number = {arXiv:1712.02029},
  eprint = {arXiv:1712.02029},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.02029},
  urldate = {2023-04-07},
  abstract = {Training deep neural networks with Stochastic Gradient Descent, or its variants, requires careful choice of both learning rate and batch size. While smaller batch sizes generally converge in fewer training epochs, larger batch sizes offer more parallelism and hence better computational efficiency. We have developed a new training approach that, rather than statically choosing a single batch size for all epochs, adaptively increases the batch size during the training process. Our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes. We analyse our approach using the standard AlexNet, ResNet, and VGG networks operating on the popular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1\% relative to training with fixed batch sizes.},
  archiveprefix = {arxiv},
  keywords = {68T05;,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,I.2.6,I.5.0,Statistics - Machine Learning},
  file = {/home/struan/Zotero/storage/8P9DACTE/Devarakonda et al. - 2018 - AdaBatch Adaptive Batch Sizes for Training Deep N.pdf;/home/struan/Zotero/storage/YXRSZKR5/1712.html}
}

@article{dongFillingVoidsElevation2020,
  title = {Filling {{Voids}} in {{Elevation Models Using}} a {{Shadow-Constrained Convolutional Neural Network}}},
  author = {Dong, Guoshuai and Huang, Weimin and Smith, William A. P. and Ren, Peng},
  year = {2020},
  month = apr,
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {17},
  number = {4},
  pages = {592--596},
  issn = {1558-0571},
  doi = {10.1109/LGRS.2019.2926530},
  abstract = {We explore the use of convolutional neural networks (CNNs) for filling voids in digital elevation models (DEM). We propose a baseline approach using a fully convolutional network to predict complete from incomplete DEMs, which is trained in a supervised fashion. We then extend this to a shadow-constrained CNN (SCCNN) by introducing additional loss functions that encourage the restored DEM to adhere to geometric constraints implied by cast shadows. At the training time, we use automatically extracted cast shadow maps and known sun directions to compute the shadow-based supervisory signal in addition to the direct DEM supervision. At the test time, our network directly predicts restored DEMs from an incomplete DEM. One key advantage of our SCCNN model is that it is characterized by both CNN data inference and geometric shadow cues. It thus avoids data restoration that may violate shadowing conditions. Both our baseline CNN and SCCNN outperform the inverse distance weighting (IDW)-based interpolation method, with the shadow supervision enabling SCCNN to obtain the best performance.},
  keywords = {Convolutional neural network (CNN),Data models,Geometry,Image restoration,Image segmentation,Interpolation,shadow geometry constraint,shadow map,Sun,Training},
  file = {/home/struan/Zotero/storage/HGJ4E3HG/Dong et al. - 2020 - Filling Voids in Elevation Models Using a Shadow-C.pdf;/home/struan/Zotero/storage/NMBUDZ4N/8789526.html}
}

@misc{dumoulinAdversariallyLearnedInference2017,
  title = {Adversarially {{Learned Inference}}},
  author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  year = {2017},
  month = feb,
  number = {arXiv:1606.00704},
  eprint = {arXiv:1606.00704},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/struan/Zotero/storage/3JQ4YP6I/Dumoulin et al. - 2017 - Adversarially Learned Inference.pdf;/home/struan/Zotero/storage/YP4VULXI/1606.html}
}

@misc{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  number = {arXiv:1603.07285},
  eprint = {arXiv:1603.07285},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.07285},
  urldate = {2023-04-11},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/struan/Zotero/storage/YI2WK88W/Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf;/home/struan/Zotero/storage/LRSIW8IN/1603.html}
}

@article{elharroussImageInpaintingReview2020,
  title = {Image {{Inpainting}}: {{A Review}}},
  shorttitle = {Image {{Inpainting}}},
  author = {Elharrouss, Omar and Almaadeed, Noor and {Al-Maadeed}, Somaya and Akbari, Younes},
  year = {2020},
  month = apr,
  journal = {Neural Processing Letters},
  volume = {51},
  number = {2},
  pages = {2007--2028},
  issn = {1573-773X},
  doi = {10.1007/s11063-019-10163-0},
  urldate = {2023-02-28},
  abstract = {Although image inpainting, or the art of repairing the old and deteriorated images, has been around for many years, it has recently gained even more popularity, because of the recent development in image processing techniques. With the improvement of image processing tools and the flexibility of digital image editing, automatic image inpainting has found important applications in computer vision and has also become an important and challenging topic of research in image processing. This paper reviews the existing image inpainting approaches, that were classified into three subcategories, sequential-based, CNN-based, and GAN-based methods. In addition, for each category, a list of methods for different types of distortion on images are presented. Furthermore, the paper also presents available datasets. Last but not least, we present the results of real evaluations of the three categories of image inpainting methods performed on the used datasets, for different types of image distortion. We also present the evaluations metrics and discuss the performance of these methods in terms of these metrics. This overview can be used as a reference for image inpainting researchers, and it can also facilitate the comparison of the methods as well as the datasets used. The main contribution of this paper is the presentation of the three categories of image inpainting methods along with a list of available datasets that the researchers can use to evaluate their proposed methodology against.},
  langid = {english},
  file = {/home/struan/Zotero/storage/YAG7CYBA/Elharrouss et al. - 2020 - Image Inpainting A Review.pdf}
}

@article{gavriilVoidFillingDigital2019,
  title = {Void {{Filling}} of {{Digital Elevation Models With Deep Generative Models}}},
  author = {Gavriil, Konstantinos and Muntingh, Georg and Barrowclough, Oliver J. D.},
  year = {2019},
  month = oct,
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {16},
  number = {10},
  pages = {1645--1649},
  issn = {1558-0571},
  doi = {10.1109/LGRS.2019.2902222},
  abstract = {In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to a superhuman performance in various tasks, such as classification, localization, and segmentation, whereas unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images. In this letter, we consider a state-of-the-art machine learning model for image inpainting, namely, a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can be successfully transferred to the setting of digital elevation models for the purpose of generating semantically plausible data for filling voids. Training, testing, and experimentation are done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority.},
  keywords = {Adaptation models,Data models,Digital elevation models,Digital elevation models (DEMs),Generative adversarial networks,Image reconstruction,Learning systems,predictive models,remote sensing,Remote sensing,unsupervised learning,Unsupervised learning},
  file = {/home/struan/Zotero/storage/WB2P4U6S/Gavriil et al. - 2019 - Void Filling of Digital Elevation Models With Deep.pdf;/home/struan/Zotero/storage/EL7B9YLU/stamp.html}
}

@article{goodfellowGenerativeAdversarialNetworks2020,
  title = {Generative Adversarial Networks},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2020},
  month = oct,
  journal = {Communications of the ACM},
  volume = {63},
  number = {11},
  pages = {139--144},
  issn = {0001-0782},
  doi = {10.1145/3422622},
  urldate = {2023-04-10},
  abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
  file = {/home/struan/Zotero/storage/L8SXFAIM/Goodfellow et al. - 2020 - Generative adversarial networks.pdf}
}

@article{grossiIntroductionArtificialNeural2008,
  title = {Introduction to Artificial Neural Networks},
  author = {Grossi, Enzo and Buscema, Massimo},
  year = {2008},
  month = jan,
  journal = {European journal of gastroenterology \& hepatology},
  volume = {19},
  pages = {1046--54},
  doi = {10.1097/MEG.0b013e3282f198a0},
  abstract = {The coupling of computer science and theoretical bases such as nonlinear dynamics and chaos theory allows the creation of 'intelligent' agents, such as artificial neural networks (ANNs), able to adapt themselves dynamically to problems of high complexity. ANNs are able to reproduce the dynamic interaction of multiple factors simultaneously, allowing the study of complexity; they can also draw conclusions on individual basis and not as average trends. These tools can offer specific advantages with respect to classical statistical techniques. This article is designed to acquaint gastroenterologists with concepts and paradigms related to ANNs. The family of ANNs, when appropriately selected and used, permits the maximization of what can be derived from available data and from complex, dynamic, and multidimensional phenomena, which are often poorly predictable in the traditional 'cause and effect' philosophy.},
  file = {/home/struan/Zotero/storage/VEJ2T9T2/Grossi and Buscema - 2008 - Introduction to artificial neural networks.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  urldate = {2023-02-28},
  file = {/home/struan/Zotero/storage/MFDMVAJE/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@article{hornHillShadingReflectance1981,
  title = {Hill Shading and the Reflectance Map},
  author = {Horn, B.K.P.},
  year = {1981},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {69},
  number = {1},
  pages = {14--47},
  issn = {1558-2256},
  doi = {10.1109/PROC.1981.11918},
  abstract = {Shaded overlays for maps give the user an immediate appreciation for the surface topography since they appeal to an important visual depth cue. A brief review of the history of manual methods is followed by a discussion of a number of methods that have been proposed for the automatic generation of shaded overlays. These techniques are compared using the reflectance map as a common representation for the dependence of tone or gray level on the orientation of surface elements.},
  keywords = {Automation,Graphics,History,Photography,Production,Reflectivity,Shadow mapping,Streaming media,Surface topography,Surface treatment},
  file = {/home/struan/Zotero/storage/D3NGDKGB/Horn - 1981 - Hill shading and the reflectance map.pdf;/home/struan/Zotero/storage/SNUJ6SYZ/1456186.html}
}

@inproceedings{jainKeysBetterImage2023,
  title = {Keys {{To Better Image Inpainting}}: {{Structure}} and {{Texture Go Hand}} in {{Hand}}},
  shorttitle = {Keys {{To Better Image Inpainting}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Jain, Jitesh and Zhou, Yuqian and Yu, Ning and Shi, Humphrey},
  year = {2023},
  pages = {208--217},
  urldate = {2023-02-28},
  langid = {english},
  file = {/home/struan/Zotero/storage/NZG6SR92/Jain et al. - 2023 - Keys To Better Image Inpainting Structure and Tex.pdf}
}

@misc{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  month = feb,
  number = {arXiv:1609.04836},
  eprint = {arXiv:1609.04836},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.04836},
  urldate = {2023-04-07},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/struan/Zotero/storage/4LNG66D3/Keskar et al. - 2017 - On Large-Batch Training for Deep Learning General.pdf;/home/struan/Zotero/storage/8M8K3BVV/1609.html}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  urldate = {2023-04-10},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {/home/struan/Zotero/storage/XT53FIT5/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@misc{LeakyReLUPyTorchDocumentation,
  title = {{{LeakyReLU}} \textemdash{} {{PyTorch}} 2.0 Documentation},
  urldate = {2023-04-11},
  howpublished = {https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html},
  file = {/home/struan/Zotero/storage/3RLCAQFH/torch.nn.LeakyReLU.html}
}

@misc{ledigPhotoRealisticSingleImage2017,
  title = {Photo-{{Realistic Single Image Super-Resolution Using}} a {{Generative Adversarial Network}}},
  author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  year = {2017},
  month = may,
  number = {arXiv:1609.04802},
  eprint = {arXiv:1609.04802},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.04802},
  urldate = {2023-03-03},
  abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/home/struan/Zotero/storage/VILB5EH3/Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf;/home/struan/Zotero/storage/NBCJ3L5D/1609.html}
}

@article{liIntegratingTopographicKnowledge2022,
  title = {Integrating Topographic Knowledge into Deep Learning for the Void-Filling of Digital Elevation Models},
  author = {Li, Sijin and Hu, Guanghui and Cheng, Xinghua and Xiong, Liyang and Tang, Guoan and Strobl, Josef},
  year = {2022},
  month = feb,
  journal = {Remote Sensing of Environment},
  volume = {269},
  pages = {112818},
  issn = {00344257},
  doi = {10.1016/j.rse.2021.112818},
  urldate = {2023-02-28},
  abstract = {Digital elevation models (DEMs) contain some of the most important data for providing terrain information and supporting environmental analyses. However, the applications of DEMs are significantly limited by data voids, which are commonly found in regions with rugged terrain. We propose a novel deep learning-based strategy called a topographic knowledge-constrained conditional generative adversarial network (TKCGAN) to fill data voids in DEMs. Shuttle Radar Topography Mission (SRTM) data with spatial resolutions of 3 and 1 arc-seconds are used in experiments to demonstrate the applicability of the TKCGAN. Qualitative topographic knowledge of valleys and ridges is transformed into new loss functions that can be applied in deep learning-based algorithms and constrain the training process. The results show that the TKCGAN outperforms other common methods in filling voids and improves the elevation and surface slope accuracy of the reconstruction results. The perfor\- mance of the TKCGAN is stable in the test areas and reduces the error in the regions with medium and high surface slopes. Furthermore, the analysis of profiles indicates that the TKCGAN achieves better performance according to a visual inspection and quantitative comparison. In addition, the proposed strategy can be applied to DEMs with different resolutions. This work is an endeavour to transform topographic knowledge into computer-processable rules and benefits future research related to terrain reconstruction and modelling.},
  langid = {english},
  file = {/home/struan/Zotero/storage/U8CM52LY/Li et al. - 2022 - Integrating topographic knowledge into deep learni.pdf;/home/struan/Zotero/storage/H4X8GSAV/S0034425721005381.html}
}

@misc{nazeriEdgeConnectGenerativeImage2019,
  title = {{{EdgeConnect}}: {{Generative Image Inpainting}} with {{Adversarial Edge Learning}}},
  shorttitle = {{{EdgeConnect}}},
  author = {Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal Z. and Ebrahimi, Mehran},
  year = {2019},
  month = jan,
  number = {arXiv:1901.00212},
  eprint = {arXiv:1901.00212},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.00212},
  urldate = {2023-02-28},
  abstract = {Over the last few years, deep learning techniques have yielded significant improvements in image inpainting. However, many of these techniques fail to reconstruct reasonable structures as they are commonly over-smoothed and/or blurry. This paper develops a new approach for image inpainting that does a better job of reproducing filled regions exhibiting fine details. We propose a two-stage adversarial model EdgeConnect that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing region (both regular and irregular) of the image, and the image completion network fills in the missing regions using hallucinated edges as a priori. We evaluate our model end-to-end over the publicly available datasets CelebA, Places2, and Paris StreetView, and show that it outperforms current state-of-the-art techniques quantitatively and qualitatively. Code and models available at: https://github.com/knazeri/edge-connect},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/struan/Zotero/storage/WG93F78X/Nazeri et al. - 2019 - EdgeConnect Generative Image Inpainting with Adve.pdf;/home/struan/Zotero/storage/6CULQD8L/1901.html}
}

@article{odenaDeconvolutionCheckerboardArtifacts2016,
  title = {Deconvolution and {{Checkerboard Artifacts}}},
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  year = {2016},
  month = oct,
  journal = {Distill},
  volume = {1},
  number = {10},
  pages = {e3},
  issn = {2476-0757},
  doi = {10.23915/distill.00003},
  urldate = {2023-04-07},
  abstract = {When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.},
  langid = {english},
  file = {/home/struan/Zotero/storage/CGIRMZ2T/deconv-checkerboard.html}
}

@article{parkNeuralProcessApproach2021,
  title = {A Neural Process Approach for Probabilistic Reconstruction of No-Data Gaps in Lunar Digital Elevation Maps},
  author = {Park, Young-Jin and Choi, Han-Lim},
  year = {2021},
  month = jun,
  journal = {Aerospace Science and Technology},
  volume = {113},
  pages = {106672},
  issn = {1270-9638},
  doi = {10.1016/j.ast.2021.106672},
  urldate = {2023-02-28},
  abstract = {With the advent of NASA's lunar reconnaissance orbiter (LRO), a large amount of high-resolution digital elevation maps (DEMs) have been constructed by using narrow-angle cameras (NACs) to characterize the Moon's surface. However, NAC DEMs commonly contain no-data gaps (voids), which makes the map less reliable. To resolve the issue, this paper provides a deep-learning-based framework for the probabilistic reconstruction of no-data gaps in NAC DEMs. The framework is built upon a state-of-the-art stochastic process model, attentive neural processes (ANP), and predicts the conditional distribution of elevation on the target coordinates (latitude and longitude) conditioned on the observed elevation data in nearby regions. Furthermore, this paper proposes sparse attentive neural processes (SANPs) that not only reduce the linear computational complexity of the ANP O(N) to the constant complexity O(K) but enhance the reconstruction performance by preventing over-fitting and over-smoothing problems. The proposed method is evaluated on three different lunar NAC DEMs with distinct geographical features, including the anthropogenic site, graben, and craterlet, demonstrating that the suggested approach successfully reconstructs no-data gaps with an uncertainty analysis while preserving the high-resolution of original NAC DEMs.},
  langid = {english},
  keywords = {Digital elevation models,Neural processes,No-data gap filling,Probabilistic inference,Self-supervised learning,Sparse approximation},
  file = {/home/struan/Zotero/storage/VSY4A6M5/Park and Choi - 2021 - A neural process approach for probabilistic recons.pdf;/home/struan/Zotero/storage/ZLEU34NB/S1270963821001826.html}
}

@misc{pathakContextEncodersFeature2016,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  year = {2016},
  month = nov,
  number = {arXiv:1604.07379},
  eprint = {arXiv:1604.07379},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.07379},
  urldate = {2023-03-03},
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/struan/Zotero/storage/29B5ZCAZ/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf;/home/struan/Zotero/storage/5P7SY7JY/1604.html;/home/struan/Zotero/storage/S4JPHGYA/1604.html}
}

@inproceedings{perezPoissonImageEditing2003,
  title = {Poisson Image Editing},
  booktitle = {{{ACM SIGGRAPH}} 2003 {{Papers}}},
  author = {P{\'e}rez, Patrick and Gangnet, Michel and Blake, Andrew},
  year = {2003},
  month = jul,
  series = {{{SIGGRAPH}} '03},
  pages = {313--318},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1201775.882269},
  urldate = {2023-04-10},
  abstract = {Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The first set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.},
  isbn = {978-1-58113-709-5},
  keywords = {guided interpolation,image gradient,interactive image editing,Poisson equation,seamless cloning,selection editing},
  file = {/home/struan/Zotero/storage/6ZZBDWG4/Pérez et al. - 2003 - Poisson image editing.pdf}
}

@article{qinImageInpaintingBased2021a,
  title = {Image Inpainting Based on Deep Learning: {{A}} Review},
  shorttitle = {Image Inpainting Based on Deep Learning},
  author = {Qin, Zhen and Zeng, Qingliang and Zong, Yixin and Xu, Fan},
  year = {2021},
  month = sep,
  journal = {Displays},
  volume = {69},
  pages = {102028},
  issn = {0141-9382},
  doi = {10.1016/j.displa.2021.102028},
  urldate = {2023-02-28},
  abstract = {Image inpainting aims to restore the pixel features of damaged parts in incomplete image and plays a key role in many computer vision tasks. Image inpainting technology based on deep learning is a major current research hotspot. To deeply understand related methods and technologies, this article combs and summarizes the latest research status in this field. Firstly, we summarize inpainting methods of different types of neural network structure based on deep learning, then analyze and study important technical improvement mechanisms. In addition, various algorithms are comprehensively reviewed from the aspects of model network structure and restoration methods. And we select some representative image inpainting methods for comparison and analysis. Finally, the current problems of image inpainting are summarized, and the future development trend and research direction are prospected.},
  langid = {english},
  keywords = {Computer vision,Generative adversarial networks (GAN),Image inpainting,Variational autoencoder (VAE)},
  file = {/home/struan/Zotero/storage/FP3BIL6J/Qin et al. - 2021 - Image inpainting based on deep learning A review.pdf;/home/struan/Zotero/storage/F2JCQJQQ/S0141938221000391.html}
}

@article{qiuVoidFillingDigital2019,
  title = {Void {{Filling}} of {{Digital Elevation Models}} with a {{Terrain Texture Learning Model Based}} on {{Generative Adversarial Networks}}},
  author = {Qiu, Zhonghang and Yue, Linwei and Liu, Xiuguo},
  year = {2019},
  month = jan,
  journal = {Remote Sensing},
  volume = {11},
  number = {23},
  pages = {2829},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2072-4292},
  doi = {10.3390/rs11232829},
  urldate = {2023-02-28},
  abstract = {Digital elevation models (DEMs) are an important information source for spatial modeling. However, data voids, which commonly exist in regions with rugged topography, result in incomplete DEM products, and thus significantly degrade DEM data quality. Interpolation methods are commonly used to fill voids of small sizes. For large-scale voids, multi-source fusion is an effective solution. Nevertheless, high-quality auxiliary source information is always difficult to retrieve in rugged mountainous areas. Thus, the void filling task is still a challenge. In this paper, we proposed a method based on a deep convolutional generative adversarial network (DCGAN) to address the problem of DEM void filling. A terrain texture generation model (TTGM) was constructed based on the DCGAN framework. Elevation, terrain slope, and relief degree composed the samples in the training set to better depict the terrain textural features of the DEM data. Moreover, the resize-convolution was utilized to replace the traditional deconvolution process to overcome the staircase in the generated data. The TTGM was trained on non-void SRTM (Shuttle Radar Topography Mission) 1-arc-second data patches in mountainous regions collected across the globe. Then, information neighboring the voids was involved in order to infer the latent encoding for the missing areas approximated to the distribution of training data. This was implemented with a loss function composed of pixel-wise, contextual, and perceptual constraints during the reconstruction process. The most appropriate fill surface generated by the TTGM was then employed to fill the voids, and Poisson blending was performed as a postprocessing step. Two models with different input sizes (64 \texttimes{} 64 and 128 \texttimes{} 128 pixels) were trained, so the proposed method can efficiently adapt to different sizes of voids. The experimental results indicate that the proposed method can obtain results with good visual perception and reconstruction accuracy, and is superior to classical interpolation methods.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,digital elevation models,terrain texture,void filling},
  file = {/home/struan/Zotero/storage/VSCTTMI6/Qiu et al. - 2019 - Void Filling of Digital Elevation Models with a Te.pdf}
}

@misc{radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2016},
  month = jan,
  number = {arXiv:1511.06434},
  eprint = {arXiv:1511.06434},
  publisher = {{arXiv}},
  urldate = {2023-03-14},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/struan/Zotero/storage/UTU7XNF3/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/home/struan/Zotero/storage/PP65IXDV/1511.html}
}

@article{reuterEvaluationVoidFilling2007,
  title = {An Evaluation of Void-filling Interpolation Methods for {{SRTM}} Data},
  author = {Reuter, H. I. and Nelson, A. and Jarvis, A.},
  year = {2007},
  month = oct,
  journal = {International Journal of Geographical Information Science},
  volume = {21},
  number = {9},
  pages = {983--1008},
  publisher = {{Taylor \& Francis}},
  issn = {1365-8816},
  doi = {10.1080/13658810601169899},
  urldate = {2023-02-28},
  abstract = {The Digital Elevation Model that has been derived from the February 2000 Shuttle Radar Topography Mission (SRTM) has been one of the most important publicly available new spatial data sets in recent years. However, the `finished' grade version of the data (also referred to as Version 2) still contains data voids (some 836,000 km2)\textemdash and other anomalies\textemdash that prevent immediate use in many applications. These voids can be filled using a range of interpolation algorithms in conjunction with other sources of elevation data, but there is little guidance on the most appropriate void-filling method. This paper describes: (i) a method to fill voids using a variety of interpolators, (ii) a method to determine the most appropriate void-filling algorithms using a classification of the voids based on their size and a typology of their surrounding terrain; and (iii) the classification of the most appropriate algorithm for each of the 3,339,913 voids in the SRTM data. Based on a sample of 1304 artificial but realistic voids across six terrain types and eight void size classes, we found that the choice of void-filling algorithm is dependent on both the size and terrain type of the void. Contrary to some previous findings, the best methods can be generalised as: kriging or inverse distance weighting interpolation for small and medium size voids in relatively flat low-lying areas; spline interpolation for small and medium-sized voids in high-altitude and dissected terrain; triangular irregular network or inverse distance weighting interpolation for large voids in very flat areas, and an advanced spline method (ANUDEM) for large voids in other terrains.},
  keywords = {DEM,DEM fusion,Interpolation methods,Void filling},
  file = {/home/struan/Zotero/storage/NMPY28FW/Reuter et al. - 2007 - An evaluation of void‐filling interpolation method.pdf}
}

@misc{salimansImprovedTechniquesTraining2016a,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  year = {2016},
  month = jun,
  number = {arXiv:1606.03498},
  eprint = {arXiv:1606.03498},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/struan/Zotero/storage/7DW5KMGS/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf;/home/struan/Zotero/storage/M4EABG6F/1606.html}
}

@article{smithLunarOrbiterLaser2010,
  title = {The {{Lunar Orbiter Laser Altimeter Investigation}} on~the~{{Lunar Reconnaissance Orbiter Mission}}},
  author = {Smith, David E. and Zuber, Maria T. and Jackson, Glenn B. and Cavanaugh, John F. and Neumann, Gregory A. and Riris, Haris and Sun, Xiaoli and Zellar, Ronald S. and Coltharp, Craig and Connelly, Joseph and Katz, Richard B. and Kleyner, Igor and Liiva, Peter and Matuszeski, Adam and Mazarico, Erwan M. and McGarry, Jan F. and {Novo-Gradac}, Anne-Marie and Ott, Melanie N. and Peters, Carlton and {Ramos-Izquierdo}, Luis A. and Ramsey, Lawrence and Rowlands, David D. and Schmidt, Stephen and Scott, V. Stanley and Shaw, George B. and Smith, James C. and Swinski, Joseph-Paul and Torrence, Mark H. and Unger, Glenn and Yu, Anthony W. and Zagwodzki, Thomas W.},
  year = {2010},
  month = jan,
  journal = {Space Science Reviews},
  volume = {150},
  number = {1},
  pages = {209--241},
  issn = {1572-9672},
  doi = {10.1007/s11214-009-9512-y},
  urldate = {2023-03-24},
  abstract = {The Lunar Orbiter Laser Altimeter (LOLA) is an instrument on the payload of NASA's Lunar Reconnaissance Orbiter spacecraft (LRO) (Chin et al., in Space Sci. Rev. 129:391\textendash 419, 2007). The instrument is designed to measure the shape of the Moon by measuring precisely the range from the spacecraft to the lunar surface, and incorporating precision orbit determination of LRO, referencing surface ranges to the Moon's center of mass. LOLA has 5 beams and operates at 28 Hz, with a nominal accuracy of 10 cm. Its primary objective is to produce a global geodetic grid for the Moon to which all other observations can be precisely referenced.},
  langid = {english},
  keywords = {Moon,Shape,Space instrumentation,Topography},
  file = {/home/struan/Zotero/storage/N7G35Q5Q/Smith et al. - 2010 - The Lunar Orbiter Laser Altimeter Investigation on.pdf}
}

@article{stolzleReconstructingOccludedElevation2022,
  title = {Reconstructing {{Occluded Elevation Information}} in {{Terrain Maps With Self-Supervised Learning}}},
  author = {St{\"o}lzle, Maximilian and Miki, Takahiro and Gerdes, Levin and Azkarate, Martin and Hutter, Marco},
  year = {2022},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {1697--1704},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3141662},
  abstract = {Accurate and complete terrain maps enhance the awareness of autonomous robots and enable safe and optimal path planning. Rocks and topography often create occlusions and lead to missing elevation information in the Digital Elevation Map (DEM). Currently, these occluded areas are either fully avoided during motion planning or the missing values in the elevation map are filled-in using traditional interpolation, diffusion or patch-matching techniques. These methods cannot leverage the high-level terrain characteristics and the geometric constraints of line of sight we humans use intuitively to predict occluded areas. We introduce a self-supervised learning approach capable of training on real-world data without a need for ground-truth information to reconstruct the occluded areas in the DEMs. We accomplish this by adding artificial occlusion to the incomplete elevation maps constructed on a real robot by performing ray casting. We first evaluate a supervised learning approach on synthetic data for which we have the full ground-truth available and subsequently move to several real-world datasets. These real-world datasets were recorded during exploration of both structured and unstructured terrain with a legged robot, and additionally in a planetary scenario on Lunar analogue terrain. We state a significant improvement compared to the baseline methods both on synthetic terrain and for the real-world datasets. Our neural network is able to run in real-time on both CPU and GPU with suitable sampling rates for autonomous ground robots. We motivate the applicability of reconstructing occlusion in elevation maps with preliminary motion planning experiments.},
  keywords = {AI-enabled robotics mapping,Casting,Image reconstruction,Neural networks,Planning,Robots,Sensors,Training},
  file = {/home/struan/Zotero/storage/4SLKWEFR/Stölzle et al. - 2022 - Reconstructing Occluded Elevation Information in T.pdf;/home/struan/Zotero/storage/UC74MPNA/9676411.html}
}

@inproceedings{suvorovResolutionRobustLargeMask2022,
  title = {Resolution-{{Robust Large Mask Inpainting With Fourier Convolutions}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},
  year = {2022},
  pages = {2149--2159},
  urldate = {2023-02-28},
  langid = {english},
  file = {/home/struan/Zotero/storage/ZHFEIQ6M/Suvorov et al. - 2022 - Resolution-Robust Large Mask Inpainting With Fouri.pdf}
}

@inproceedings{tranGeneratingDigitalTerrain2010,
  title = {Generating Digital Terrain Models Using Lroc Nac Images},
  booktitle = {International {{Archives}} of the {{Photogrammetry}}, {{Remote Sensing}} and {{Spatial Information Sciences}} - {{ISPRS Archives}}},
  author = {Tran, T. and Rosiek, M.R. and Beyer, R.A. and Mattson, S. and {Howington-Kraus}, E. and Robinson, M.S. and Archinal, B.A. and Edmundson, K. and Harbour, D. and Anderson, E.},
  year = {2010},
  volume = {38},
  issn = {1682-1750},
  abstract = {The Lunar Reconnaissance Orbiter Camera (LROC) consists of one Wide Angle Camera (WAC) for synoptic multispectral imaging and two Narrow Angle Cameras (NAC) to provide high-resolution images (0.5 to 2.0 m pixel scale) of key targets. LROC was not designed as a stereo system, but can obtain stereo pairs through images acquired from two orbits (with at least one off-nadir slew). Off-nadir rolls interfere with the data collection of the other instruments, so during the nominal mission LROC slew opportunities are limited to three per day.},
  langid = {english},
  keywords = {DTM,LROC,Mapping,Moon,Topography},
  file = {/home/struan/Zotero/storage/52RPTHHQ/Tran et al. - 2010 - Generating digital terrain models using lroc nac i.pdf;/home/struan/Zotero/storage/CQDLK3QZ/display.html}
}

@misc{wengGANWGAN2019,
  title = {From {{GAN}} to {{WGAN}}},
  author = {Weng, Lilian},
  year = {2019},
  month = apr,
  number = {arXiv:1904.08994},
  eprint = {arXiv:1904.08994},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.08994},
  urldate = {2023-02-28},
  abstract = {This paper explains the math behind a generative adversarial network (GAN) model and why it is hard to be trained. Wasserstein GAN is intended to improve GANs' training by adopting a smooth metric for measuring the distance between two probability distributions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/struan/Zotero/storage/YGNJLDP6/Weng - 2019 - From GAN to WGAN.pdf;/home/struan/Zotero/storage/JM6VP3RR/1904.html}
}

@misc{WhatOptimalBatch,
  title = {What's the {{Optimal Batch Size}} to {{Train}} a {{Neural Network}}?},
  journal = {Weights \& Biases},
  urldate = {2023-04-07},
  abstract = {Weights \& Biases, developer tools for machine learning},
  howpublished = {https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU},
  langid = {english},
  file = {/home/struan/Zotero/storage/A9VKJJZW/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU.html}
}

@inproceedings{yuFreeFormImageInpainting2019,
  title = {Free-{{Form Image Inpainting With Gated Convolution}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
  year = {2019},
  pages = {4471--4480},
  urldate = {2023-02-28},
  file = {/home/struan/Zotero/storage/REX92F64/Yu et al. - 2019 - Free-Form Image Inpainting With Gated Convolution.pdf}
}

@misc{yuGenerativeImageInpainting2018,
  title = {Generative {{Image Inpainting}} with {{Contextual Attention}}},
  author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
  year = {2018},
  month = mar,
  number = {arXiv:1801.07892},
  eprint = {arXiv:1801.07892},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative\_inpainting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/home/struan/Zotero/storage/Z79ZK4ZT/Yu et al. - 2018 - Generative Image Inpainting with Contextual Attent.pdf;/home/struan/Zotero/storage/GJBNAJJ6/1801.html}
}

@misc{yuMultiScaleContextAggregation2016,
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  year = {2016},
  month = apr,
  number = {arXiv:1511.07122},
  eprint = {arXiv:1511.07122},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.07122},
  urldate = {2023-02-28},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/struan/Zotero/storage/25RDVSZW/Yu and Koltun - 2016 - Multi-Scale Context Aggregation by Dilated Convolu.pdf;/home/struan/Zotero/storage/FESNZ6ZH/1511.html}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image},
  file = {/home/struan/Zotero/storage/R6UDFNYM/Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}

@article{zhangVoidFillingBased2020,
  title = {{{DEM Void Filling Based}} on {{Context Attention Generation Model}}},
  author = {Zhang, Chunsen and Shi, Shu and Ge, Yingwei and Liu, Hengheng and Cui, Weihong},
  year = {2020},
  month = dec,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {9},
  number = {12},
  pages = {734},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2220-9964},
  doi = {10.3390/ijgi9120734},
  urldate = {2023-02-28},
  abstract = {The digital elevation model (DEM) generates a digital simulation of ground terrain in a certain range with the usage of 3D point cloud data. It is an important source of spatial modeling information. Due to various reasons, however, the generated DEM has data holes. Based on the algorithm of deep learning, this paper aims to train a deep generation model (DGM) to complete the DEM void filling task. A certain amount of DEM data and a randomly generated mask are taken as network inputs, along which the reconstruction loss and generative adversarial network (GAN) loss are used to assist network training, so as to perceive the overall known elevation information, in combination with the contextual attention layer, and generate data with reliability to fill the void areas. The experimental results have managed to show that this method has good feature expression and reconstruction accuracy in DEM void filling, which has been proven to be better than that illustrated by the traditional interpolation method.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {context attention layer,deep generative model,deep learning,digital elevation model,void filling},
  file = {/home/struan/Zotero/storage/ZNHVN9ZA/Zhang et al. - 2020 - DEM Void Filling Based on Context Attention Genera.pdf}
}
